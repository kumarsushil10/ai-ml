{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ae5fd5f",
   "metadata": {},
   "source": [
    "# Simple Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5188f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun1(input1):\n",
    "    return input1 + \"from fun1\"\n",
    "\n",
    "def fun2(input2):\n",
    "    return input2 + \" from fun2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac78f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow1 = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc34d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow1.add_node(\"fun1\", fun1)\n",
    "workflow1.add_node(\"fun2\", fun2)\n",
    "\n",
    "workflow1.add_edge(\"fun1\", \"fun2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e20d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow1.set_entry_point(\"fun1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7afe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow1.set_finish_point(\"fun2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d494ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "app1 = workflow1.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56caaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "app1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app1.invoke(\"Hello World, Welcome \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdebaf74",
   "metadata": {},
   "source": [
    "# Workflow with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langgraph.graph import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51be3f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input):\n",
    "    llm = ChatGroq(model=\"gemma2-9b-it\")\n",
    "    return llm.invoke(input).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbabaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_case(input):\n",
    "    return input.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a255ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow2 = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e812507",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow2.add_node(\"model\", model)\n",
    "workflow2.add_node(\"upper_case\", upper_case)\n",
    "\n",
    "workflow2.add_edge(\"model\", \"upper_case\")\n",
    "\n",
    "workflow2.set_entry_point(\"model\")\n",
    "workflow2.set_finish_point(\"upper_case\")\n",
    "\n",
    "app2 = workflow2.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186588f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "app2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600b6cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "app2.invoke(\"who is the president of the India?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbe2ed4",
   "metadata": {},
   "source": [
    "# Workflow with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a43377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS \n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langgraph.graph import Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebfbb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"arxiv_mcp.pdf\"\n",
    "loader = PyPDFLoader(path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a60f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splitted_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccaedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emebeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "db = FAISS.from_documents(splitted_documents, emebeddings)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5e07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a helpful assistant. You will be provided with a question and a context. Answer the question based on the context.\n",
    "    Context: {context}\n",
    "    Question: {input}\n",
    "    \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1461acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca49536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_model(input):\n",
    "    documents_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, documents_chain)\n",
    "    response = rag_chain.invoke({\"input\":input})\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf7bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_model(input):\n",
    "    llm = ChatGroq(model=\"gemma2-9b-it\")\n",
    "    return llm.invoke(input).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d039490",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow3 = Graph()\n",
    "workflow3.add_node(\"RAG_model\", rag_model)\n",
    "workflow3.add_node(\"LLM_model\", llm_model)\n",
    "workflow3.add_edge(\"RAG_model\", \"LLM_model\")\n",
    "workflow3.set_entry_point(\"RAG_model\")\n",
    "workflow3.set_finish_point(\"LLM_model\")\n",
    "app3 = workflow3.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "app3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6785fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = app3.invoke(\"What is the main theme of the paper?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2709c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "try:\n",
    "    display(Image(app3.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ca8bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"What is the main theme of the paper?\"\n",
    "for output in app3.stream(input):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d800c8c7",
   "metadata": {},
   "source": [
    "# Workflow with Conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6eee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe543c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"arxiv_mcp.pdf\"\n",
    "loader = PyPDFLoader(path)\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splitted_documents = text_splitter.split_documents(documents)\n",
    "emebeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "db = FAISS.from_documents(splitted_documents, emebeddings)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82988542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel , Field\n",
    "class TopicSelectionParser(BaseModel):\n",
    "    Topic: str = Field(description='Selected Topic')\n",
    "    Reasoning: str = Field(description='Reasoning behind topic selection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6048d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=TopicSelectionParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6284ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq.chat_models import ChatGroq\n",
    "llm = ChatGroq(model=\"gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b597cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "def agent(state):\n",
    "    message=state[\"messages\"]\n",
    "    question=message[-1]\n",
    "    print(question)\n",
    "    \n",
    "    template=\"\"\"\n",
    "    Your task is to classify the given user query into one of the following categories: [model context protocol(MCP), Not Related]. \n",
    "    Only respond with the category name and nothing else.\n",
    "    User query: {question}\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(template=template,\n",
    "                            input_variables=[question],\n",
    "                            partial_variables={\"format_instructions\" : parser.get_format_instructions()})\n",
    "    \n",
    "    chain =  prompt | llm | parser\n",
    "    response = chain.invoke({\"question\":question,\"format_instructions\" : parser.get_format_instructions()})\n",
    "\n",
    "    print(response)\n",
    "    return {\"messages\": [response.Topic]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e0399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state):\n",
    "    print('-> Router ->')\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    print(last_message)\n",
    "    if 'model context protocol'.lower() or 'mcp'.lower() in last_message:\n",
    "        return 'RAG Call'\n",
    "    else:\n",
    "        return 'LLM Call'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "def rag_model(state):\n",
    "    print('-> Calling RAG ->')\n",
    "    messages = state['messages']\n",
    "    question = messages[0]\n",
    "    print(question)\n",
    "    template = \"\"\"Answer the question based only on the following context:{context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    print(prompt)\n",
    "\n",
    "    retrieval_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "    result = retrieval_chain.invoke(question)\n",
    "    return  {\"messages\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4687cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_model(state):\n",
    "    print('-> Calling LLM ->')\n",
    "\n",
    "    messages = state['messages']\n",
    "    question = messages[0] ## Fetching the user question\n",
    "\n",
    "    # Normal LLM call\n",
    "    complete_query = \"Anwer the follow question with your knowledge of the real world. Following is the user question: \" + question\n",
    "    response = llm.invoke(complete_query)\n",
    "    return {\"messages\": [response.content]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a926e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Sequence, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a505828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph,END\n",
    "workflow4 = StateGraph(AgentState) ### StateGraph with AgentState\n",
    "\n",
    "workflow4.add_node(\"agent\", agent)\n",
    "workflow4.add_node(\"RAG\", rag_model)\n",
    "workflow4.add_node(\"LLM\", llm_model)\n",
    "\n",
    "\n",
    "workflow4.set_entry_point(\"agent\")\n",
    "workflow4.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    router,\n",
    "    {\n",
    "        \"RAG Call\": \"RAG\",\n",
    "        \"LLM Call\": \"LLM\",\n",
    "    }\n",
    ")\n",
    "workflow4.add_edge(\"RAG\",END)\n",
    "workflow4.add_edge(\"LLM\",END)\n",
    "app4=workflow4.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6742b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "try:\n",
    "    display(Image(app4.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf466cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [\"Tell me about the model context protocol\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d4366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = app4.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb93c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.get(\"messages\")[-1])  # Print the last message in the output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
